# -------------------
# Data Settings
# -------------------
# Defines the source data and how it should be processed.
data:
  project_dir: .
  input_path: input.txt
  remove_duplicates: True     # If true, removes duplicate lines from the input file before splitting.
  no_duplicates_path: data/no_duplicates.txt
  test_size: 0.1              # The proportion of the dataset to be held out for the test set.
  train_path: data/train.txt
  test_path: data/test.txt
  max_word_per_sentence: 128 # Sentences with more words than this will be skipped.

# -------------------
# Output Settings
# -------------------
# Defines the final output path for the production-ready model.
final_model_path: out/model.pt
final_tokenizer_path: out/tokenizer.json

# -------------------
# Tokenizer Settings
# -------------------
# Configures the sentencepiece tokenizer.
tokenizer:
  save_path: data/tokenizer.json # Path to save the trained tokenizer file.
  vocab_size: 35000
  keep_punctuation: False     # If false, punctuation will be stripped from the text. Usually recommended.
  keep_accents: True          # If true, characters with accents (e.g., é, ö) are kept. If false, they are decomposed (e.g., 'é' -> 'e'). Recommended for non-English languages.

# -------------------
# Hyperparameter Optimization Settings (Optuna)
# -------------------
# Defines the search space and execution for the hyperparameter search.
optimize:
  epochs: 15
  n_trials: 100                # Total number of Optuna trials to run for the hyperparameter search.
  storage_name: "sqlite:///data/my_study.db"
  study_name: "encoder-optimization-v1"
  max_train_batches_per_epoch: 1250 # Limits the number of training batches per epoch during optimization to speed up trials. Set to -1 for no limit.
  max_eval_batches_per_epoch: 250  # Limits the number of evaluation batches per epoch during optimization.
  batch_size: 32            # Fixed batch size for all optimization trials.
  accumulation_steps: 1     # Fixed number of gradient accumulation steps. actual batch_size is batch_size x accumulation_steps
  
  # --- Search Space ---
  optimizer_name: # A list of optimizers for Optuna to choose from.
    - AdamW
    - Adam
    - RMSprop
  learning_rate: # The search range for the learning rate.
    min: 1e-6
    max: 1e-4
  embedding_dim: # The model's embedding dimension. Using standard, proven sizes.
    - 256
    - 512
    - 768
  num_attention_heads: # The number of attention heads. For every combination, embedding_dim must be divisible by num_attention_heads.
    - 4
    - 8
    - 16
  num_encoder_layers: # The number of layers in the Transformer encoder. More layers increase model size and computation time.
    min: 4
    max: 8
    step: 2
  dropout: # The search range for the dropout rate. 0.1 is a common baseline.
    min: 0.05
    max: 0.2

# -------------------
# Final Training Settings
# -------------------
# Settings for training the model with a fixed set of hyperparameters.
train:
  epochs: 12
  batch_size: 32
  # Performs gradient accumulation. The effective batch size will be `batch_size * accumulation_steps`.
  # This allows for larger effective batch sizes without increasing memory usage.
  # `batch_size` must be a multiple of this value.
  accumulation_steps: 1
  max_train_batches: -1         # Limits the number of training batches per epoch. Set to -1 for no limit.
  max_eval_batches: -1          # Limits the number of evaluation batches per epoch. Set to -1 for no limit.
  model_save_path: models/model.pt # Path to save the final trained model weights.
  config_save_path: models/model.cfg # Path to save the configuration of the final trained model.
  scheduler_patience: 2           # Scheduler: Number of epochs with no improvement after which learning rate will be reduced.
  scheduler_factor: 0.5           # Scheduler: Factor by which the learning rate will be reduced. new_lr = lr * factor.
  
  override_with_custom_params:
    enabled: False              # If true, the training process will use the parameters defined below instead of the best ones found by Optuna.
    optimizer_name: RMSprop     # The optimizer. Possible values: "Adam", "AdamW", "RMSprop"
    embedding_dim: 512          # The model's embedding dimension.
    num_attention_heads: 8      # The number of attention heads.
    num_encoder_layers: 6       # The number of layers in the Transformer encoder.
    dropout: 0.15               # The dropout rate.
    learning_rate: 1e-5         # The learning rate.

# -------------------
# Fine-Tuning Settings
# -------------------
# Settings for fine-tuning a pre-trained model.
finetune:
  epochs: 12
  # The ratio of BOTTOM layers to freeze during fine-tuning. 0.25 means the bottom 25% of layers will be frozen.
  # The embedding layer is always frozen in addition to this.
  batch_size: 256            # Fixed batch size 
  small_batch_size: 32
  max_train_batches: -1         # Limits the number of training batches per epoch. Set to -1 for no limit.
  max_eval_batches: -1          # Limits the number of evaluation batches per epoch. Set to -1 for no limit.
  freeze_layer_ratio: 0.25
  loss:
    name: "debiased_contrastive" # "info_nce" or "debiased_contrastive"
    temperature: 0.07
    p: 0.1 # Only used for debiased_contrastive
  model_save_path: models/model_finetuned.pt # Path to save the final fine-tuned model weights.
  scheduler_patience: 1           # Scheduler: Number of epochs with no improvement after which learning rate will be reduced.
  scheduler_factor: 0.5           # Scheduler: Factor by which the learning rate will be reduced. new_lr = lr * factor.
  
  override_with_custom_params:
    # If true, fine-tuning will use the parameters below instead of the best ones found by Optuna.
    # A pre-trained model is required to start fine-tuning.
    enabled: False
    optimizer_name: RMSprop     # The optimizer. Possible values: "Adam", "AdamW", "RMSprop"
    dropout: 0.15               # The dropout rate.
    learning_rate: 1e-5         # The learning rate.

# -------------------
# QA Fine-Tuning Settings
# -------------------
# Settings for fine-tuning the model on QA data.
finetuning_qa:
  qa_yaml_path: data/hotpotqa_qa_data.yaml # Path to the QA YAML data file
  epochs: 10
  batch_size: 1 # Each batch corresponds to one article
  temperature: 0.07
  debiasing_lambda: 0.1
  model_save_path: models/model_finetuned_qa.pt # Path to save the QA fine-tuned model weights.
  scheduler_patience: 1
  scheduler_factor: 0.5